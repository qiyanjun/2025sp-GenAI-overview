---
layout: post
title: advanced LLM - for math reasoning / coding   
lecture: 
lectureVersion: next
extraContent: 
notes: team-2
video: team-2
tags:
- 
desc: 2025-S4
term: 2025-seminarRead
categories:
- 
---


In this session, our readings cover: 

## Required Readings: 

Agent Laboratory: Using LLM Agents as Research Assistants

Don't Do RAG: When Cache-Augmented Generation is All You Need for Knowledge Tasks
  

#### Language Models for Code Optimization: Survey, Challenges and Future Directions

[Submitted on 2 Jan 2025 (v1), last revised 3 Jan 2025 (this version, v2)]

Jingzhi Gong, Vardan Voskanyan, Paul Brookes, Fan Wu, Wei Jie, Jie Xu, Rafail Giavrimis, Mike Basios, Leslie Kanthan, Zheng Wang
Language models (LMs) built upon deep neural networks (DNNs) have recently demonstrated breakthrough effectiveness in software engineering tasks such as code generation, completion, and repair. This has paved the way for the emergence of LM-based code optimization techniques, which are crucial for enhancing the performance of existing programs, such as accelerating program execution time. However, a comprehensive survey dedicated to this specific application has been lacking. To fill this gap, we present a systematic literature review of over 50 primary studies, identifying emerging trends and addressing 11 specialized questions. Our findings reveal five critical open challenges, such as balancing model complexity with practical usability, cross-language/performance generalizability, and building trust in AI-driven solutions. Furthermore, we provide eight future research directions to facilitate more efficient, robust, and reliable LM-based code optimization. Thereby, this study aims to provide actionable insights and foundational references for both researchers and practitioners in this rapidly evolving field.

## More Readings: 

