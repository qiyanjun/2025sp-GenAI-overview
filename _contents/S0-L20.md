---
layout: post
title: Agent needs Efficiency - Model Serving    
lecture: 
lectureVersion: next
extraContent: 
notes: 
video: team-6
tags:
- 
desc: 2025-S4
term: 2025-seminarRead
categories:
- 
---


In this session, our readings cover: 

## Required Readings: 


 vLLM, continuous batching, chunked prefill, fair scheduling, KV cache management, and disaggregated serving,


#### Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems
+ Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Hongyi Jin, Tianqi Chen, Zhihao Jia
+ In the rapidly evolving landscape of artificial intelligence (AI), generative large language models (LLMs) stand at the forefront, revolutionizing how we interact with our data. However, the computational intensity and memory consumption of deploying these models present substantial challenges in terms of serving efficiency, particularly in scenarios demanding low latency and high throughput. This survey addresses the imperative need for efficient LLM serving methodologies from a machine learning system (MLSys) research perspective, standing at the crux of advanced AI innovations and practical system optimizations. We provide in-depth analysis, covering a spectrum of solutions, ranging from cutting-edge algorithmic modifications to groundbreaking changes in system designs. The survey aims to provide a comprehensive understanding of the current state and future directions in efficient LLM serving, offering valuable insights for researchers and practitioners in overcoming the barriers of effective LLM deployment, thereby reshaping the future of AI.
+ [https://arxiv.org/pdf/2312.15234](https://arxiv.org/pdf/2312.15234)
  
 

#### A Survey on Model Compression for Large Language Models
+ [Submitted on 15 Aug 2023 (v1), last revised 30 Jul 2024 (this version, v4)]
+ Xunyu Zhu, Jian Li, Yong Liu, Can Ma, Weiping Wang
+ [https://arxiv.org/abs/2308.07633](https://arxiv.org/abs/2308.07633)
+ Large Language Models (LLMs) have transformed natural language processing tasks successfully. Yet, their large size and high computational needs pose challenges for practical use, especially in resource-limited settings. Model compression has emerged as a key research area to address these challenges. This paper presents a survey of model compression techniques for LLMs. We cover methods like quantization, pruning, and knowledge distillation, highlighting recent advancements. We also discuss benchmarking strategies and evaluation metrics crucial for assessing compressed LLMs. This survey offers valuable insights for researchers and practitioners, aiming to enhance efficiency and real-world applicability of LLMs while laying a foundation for future advancements.
Comments:	Accepted for publication in TACL; a pre-MIT Press publication version




## More Readings: 

