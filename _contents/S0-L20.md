---
layout: post
title: Agent - Tooling   
lecture: 
lectureVersion: next
extraContent: 
notes: 
video: team-6
tags:
- 
desc: 2025-S4
term: 2025-seminarRead
categories:
- 
---


In this session, our readings cover: 

## Required Readings: 


### Agent Tools: 
+ [https://langchain-ai.github.io/langgraph/tutorials/introduction/](https://langchain-ai.github.io/langgraph/tutorials/introduction/)

+ [https://huggingface.co/docs/transformers/en/agents](https://huggingface.co/docs/transformers/en/agents)

+ [AutoGen](https://github.com/ag2ai/ag2?tab=readme-ov-file)

+ 



#### Agent-as-a-Judge: Evaluate Agents with Agents
+ [Submitted on 14 Oct 2024 (v1), last revised 16 Oct 2024 (this version, v2)]
+ Mingchen Zhuge, Changsheng Zhao, Dylan Ashley, Wenyi Wang, Dmitrii Khizbullin, Yunyang Xiong, Zechun Liu, Ernie Chang, Raghuraman Krishnamoorthi, Yuandong Tian, Yangyang Shi, Vikas Chandra, JÃ¼rgen Schmidhuber
+ Contemporary evaluation techniques are inadequate for agentic systems. These approaches either focus exclusively on final outcomes -- ignoring the step-by-step nature of agentic systems, or require excessive manual labour. To address this, we introduce the Agent-as-a-Judge framework, wherein agentic systems are used to evaluate agentic systems. This is an organic extension of the LLM-as-a-Judge framework, incorporating agentic features that enable intermediate feedback for the entire task-solving process. We apply the Agent-as-a-Judge to the task of code generation. To overcome issues with existing benchmarks and provide a proof-of-concept testbed for Agent-as-a-Judge, we present DevAI, a new benchmark of 55 realistic automated AI development tasks. It includes rich manual annotations, like a total of 365 hierarchical user requirements. We benchmark three of the popular agentic systems using Agent-as-a-Judge and find it dramatically outperforms LLM-as-a-Judge and is as reliable as our human evaluation baseline. Altogether, we believe that Agent-as-a-Judge marks a concrete step forward for modern agentic systems -- by providing rich and reliable reward signals necessary for dynamic and scalable self-improvement.
Comments:	The project can be found at this https URL. The dataset is released at this https URL

## More Readings: 

