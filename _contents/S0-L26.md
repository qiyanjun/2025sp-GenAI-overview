---
layout: post
title: Small foundation models  
lecture: 
lectureVersion: next
extraContent: 
notes: team-6
video: team-6
tags:
- 
desc: 2025-S4
term: 2025-seminarRead
categories:
- 
---


In this session, our readings cover: 

## Required Readings: 


#### Small Language Models (SLMs) Can Still Pack a Punch: A survey
Shreyas Subramanian, Vikram Elango, Mecit Gungor
As foundation AI models continue to increase in size, an important question arises - is massive scale the only path forward? This survey of about 160 papers presents a family of Small Language Models (SLMs) in the 1 to 8 billion parameter range that demonstrate smaller models can perform as well, or even outperform large models. We explore task agnostic, general purpose SLMs, task-specific SLMs and techniques to create SLMs that can guide the community to build models while balancing performance, efficiency, scalability and cost. Furthermore we define and characterize SLMs' effective sizes, representing increased capability with respect to LLMs.
  

#### rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking


## More Readings: 

