---
layout: post
title:  Model serving - Efficiency 
lecture: 
lectureVersion: next
extraContent: 
notes: 
video: team-6
tags:
- 
desc: 2025-S4
term: 2025-seminarRead
categories:
- 
---



## Required Readings: 


#### Generative AI on the Edge: Architecture and Performance Evaluation
+ Zeinab Nezami, Maryam Hafeez, Karim Djemame, Syed Ali Raza Zaidi
+ [Submitted on 18 Nov 2024]
+ 6G's AI native vision of embedding advance intelligence in the network while bringing it closer to the user requires a systematic evaluation of Generative AI (GenAI) models on edge devices. Rapidly emerging solutions based on Open RAN (ORAN) and Network-in-a-Box strongly advocate the use of low-cost, off-the-shelf components for simpler and efficient deployment, e.g., in provisioning rural connectivity. In this context, conceptual architecture, hardware testbeds and precise performance quantification of Large Language Models (LLMs) on off-the-shelf edge devices remains largely unexplored. This research investigates computationally demanding LLM inference on a single commodity Raspberry Pi serving as an edge testbed for ORAN. We investigate various LLMs, including small, medium and large models, on a Raspberry Pi 5 Cluster using a lightweight Kubernetes distribution (K3s) with modular prompting implementation. We study its feasibility and limitations by analyzing throughput, latency, accuracy and efficiency. Our findings indicate that CPU-only deployment of lightweight models, such as Yi, Phi, and Llama3, can effectively support edge applications, achieving a generation throughput of 5 to 12 tokens per second with less than 50\% CPU and RAM usage. We conclude that GenAI on the edge offers localized inference in remote or bandwidth-constrained environments in 6G networks without reliance on cloud infrastructure.



#### Harnessing the Power of Multiple Minds: Lessons Learned from LLM Routing
+ KV Aditya Srivatsa, Kaushal Kumar Maurya, Ekaterina Kochmar
+ With the rapid development of LLMs, it is natural to ask how to harness their capabilities efficiently. In this paper, we explore whether it is feasible to direct each input query to a single most suitable LLM. To this end, we propose LLM routing for challenging reasoning tasks. Our extensive experiments suggest that such routing shows promise but is not feasible in all scenarios, so more robust approaches should be investigated to fill this gap.

