---
layout: post
title: Introduction
lecture: 
lectureVersion: current
extraContent: 
notes: instructor
video: on nlp basics
tags:
- BasicLLM
desc: 2025-S0
term: 2025-seminarRead
categories:
- FMBasic
---



## Readings: 

#### Basics of ML and DL: 
- [URL](https://qiyanjun.github.io/2022sp-UVA-CS-MachineLearningDeep/)

#### Basics of NLP 
- [URL](https://qiyanjun.github.io/2022sp-UVA-CS-MachineLearningDeep//Lectures/S3-deepNNtext.pdf)
- Typical NLP tasks / Challenges / Pipeline
- f() on natural language
  + Before Deep NLP (Pre 2012) • (BOW / LSI / Topic Modeling LDA )
  + Word2Vec (2013-2016) • (GloVe/ FastText)
  + Recurrent NN (2014-2016) • LSTM
  + Seq2Seq
  + Attention 
  + Self-Attention (2016 – now )
  + Transformer (attention only Seq2Seq)
  + BERT / RoBERTa/ XLNet/ GPT / ...


+ A good code walk through on transformer at [URL](https://nlp.seas.harvard.edu/annotated-transformer/)


## Extra Readings: 


#### Emergent Abilities of Large Language Models 
  + [ URL](https://arxiv.org/abs/2206.07682) 
  + "an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models."|

#### Language Models are Few-Shot Learners 
  + [ URL](https://arxiv.org/abs/2005.14165) 
  + "GPT-3, 175B autoregerssive LLM;  show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches."|



#### Generative AI: Perspectives from Stanford HAI
 + https://hai.stanford.edu/generative-ai-perspectives-stanford-hai 



<!--excerpt.start-->

